We will run this program locally on our computer, so all files are available to all workers. But in a real distributed system this is not the case. How they handle this problem ? They use  adistributed file system like GFS (google file system) that will make all files available to all nodes in the cluster.



Map phase :

we will start having 1 map task per input file. Reduce task is defined by nReduce (10 per default)


TODO:

CHECK FOR ERRORS

File Handling: You might need to add logic for managing intermediate files between map and reduce phases.
